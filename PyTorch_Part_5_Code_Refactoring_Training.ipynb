{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "PyTorch Part 5 Code Refactoring Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KunYp1e_SKFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6f050e8b-812c-4b41-eb3f-876a426bf9e4"
      },
      "source": [
        "import torch.utils.tensorboard\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g_WNc7NCSKFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "torch.set_printoptions(linewidth=120)\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9H5aOs7ASKFj",
        "colab_type": "code",
        "outputId": "914df1a5-314f-4b6c-e3c6-6e39dedc576c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n",
            "0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AY6Zn_E1SKFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_nums(preds,labels):\n",
        "    return (preds.argmax(dim=1).eq(labels).sum().item())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PKWb10qySKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module): #Line 1\n",
        "    def __init__(self):\n",
        "        super().__init__() # Line 3\n",
        "        \n",
        "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "        \n",
        "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)# 12 comnes from output of 12 channel in conv2\n",
        "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
        "        \n",
        "        self.out=nn.Linear(in_features=60, out_features=10)\n",
        "        \n",
        "    def forward(self, t):\n",
        "        # (1) Input Layer\n",
        "        #t=t\n",
        "        \n",
        "        # (2) hidden conv1 Layer\n",
        "        t=self.conv1(t)\n",
        "        t=F.relu(t)\n",
        "        t=F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "        \n",
        "        # (3) hidden conv2 Layer\n",
        "        t=self.conv2(t)\n",
        "        t=F.relu(t)\n",
        "        t=F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "        \n",
        "        # (4) hidden linear1 layer\n",
        "        t=t.reshape(-1, 12*4*4) # Need to flatten before using FC layer \n",
        "        t=self.fc1(t)\n",
        "        t=F.relu(t)\n",
        "        \n",
        "        # (5) Hidden linear2 layer\n",
        "        t=self.fc2(t)\n",
        "        t=F.relu(t)\n",
        "        \n",
        "        # (6) Output layer\n",
        "        t=self.out(t)\n",
        "        # Avoiding softmax because we will use cross entropy loss fucntion which has built in softmax fucntion.\n",
        "        #t=F.softmax(t, dim=1) \n",
        "        # THhis alllows the network to not compute the additiional operation while network is been infrenced. But computaion will of course work.\n",
        "        \n",
        "        \n",
        "        \n",
        "        return t\n",
        "network=Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "js4DVkx1SKF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set=torchvision.datasets.FashionMNIST(\n",
        "    root='./data/FashionMINIST',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose( # For composition of transforamtion here onlu one transformation is sufficient \n",
        "        [transforms.ToTensor()]\n",
        "        )\n",
        ")\n",
        "train_loader=torch.utils.data.DataLoader(\n",
        "    train_set, \n",
        "    batch_size=100, #Default batach size =1\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtro2lMklsma",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Tuning & Experimentation auto mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPKFJc3SBqZS",
        "colab_type": "text"
      },
      "source": [
        "* Earlier process of training was inefficent, now we will make them effcient using runs\n",
        "### Objective:\n",
        "\n",
        "1. We want automatic addition of parameters to set just by defining them.\n",
        "2. We want program to unpack the parameter by itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVZe5oZXpxWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import  OrderedDict\n",
        "from collections import namedtuple\n",
        "from itertools import product"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqbeaWekDHnZ",
        "colab_type": "code",
        "outputId": "9bd95fba-ba55-4762-841c-359530b55941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parameters=dict(\n",
        "    lr=[0.01,0.001,0.0001,0.00001],\n",
        "    batch_size=[100,1000,10000],\n",
        "    shuffle=[False]\n",
        ")\n",
        "parameters.keys()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['lr', 'batch_size', 'shuffle'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B75qbC_PDWez",
        "colab_type": "code",
        "outputId": "391c048b-7bac-4dac-dc9f-9e2a3f34b86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parameters.values()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([[0.01, 0.001, 0.0001, 1e-05], [100, 1000, 10000], [False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDw4e7A2Dkdp",
        "colab_type": "code",
        "outputId": "80538600-8080-4240-cb9e-9237640c9697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# namedtuple help us access the tuple by name also, rather then by only index\n",
        "# Example:\n",
        "Point = namedtuple('Point', ['x', 'y'])\n",
        "p=Point(11,12)\n",
        "p # Named tuple behave as tuple but also as function, they can accept parameters"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Point(x=11, y=12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtTsqVbyGfKH",
        "colab_type": "code",
        "outputId": "801ddf32-e864-4591-cebb-300505a63c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p.x+p.y"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_7GIk6MGY1I",
        "colab_type": "code",
        "outputId": "c30273f2-b32a-4443-973e-8a6e2ecc6a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Run=namedtuple('Run',parameters.keys())\n",
        "Run"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Run"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VnGY47vD57-",
        "colab_type": "code",
        "outputId": "8c82127e-4298-465e-d629-ef6b1ee0d83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "runs=[]\n",
        "for v in product(*parameters.values()):\n",
        "  runs.append(Run(*v))# Star \"*\" helps us to pass the value as paramters rather then the tuple itself\n",
        "runs"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Run(lr=0.01, batch_size=100, shuffle=False),\n",
              " Run(lr=0.01, batch_size=1000, shuffle=False),\n",
              " Run(lr=0.01, batch_size=10000, shuffle=False),\n",
              " Run(lr=0.001, batch_size=100, shuffle=False),\n",
              " Run(lr=0.001, batch_size=1000, shuffle=False),\n",
              " Run(lr=0.001, batch_size=10000, shuffle=False),\n",
              " Run(lr=0.0001, batch_size=100, shuffle=False),\n",
              " Run(lr=0.0001, batch_size=1000, shuffle=False),\n",
              " Run(lr=0.0001, batch_size=10000, shuffle=False),\n",
              " Run(lr=1e-05, batch_size=100, shuffle=False),\n",
              " Run(lr=1e-05, batch_size=1000, shuffle=False),\n",
              " Run(lr=1e-05, batch_size=10000, shuffle=False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n7QGI8eHVgA",
        "colab_type": "text"
      },
      "source": [
        "#### Making class of above steps we have:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Y0-plyE_U0",
        "colab_type": "text"
      },
      "source": [
        "* The RunBuilder class encapsulates the get_runs function. \n",
        "* This is considered to be better design than simply having the function sitting inside the global scope of the program. \n",
        "* This becomes especially true if there are multiple functions that logically belong together.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebrqiMtPBXUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RunBuilder():\n",
        "  @staticmethod \n",
        "  # This allows us to call the function without calling the class, hence saving memory. \n",
        "  # This is done becuase we do not want global functions and neither extra class objects.  \n",
        "  def get_runs(parameters):\n",
        "    Run=namedtuple(\"Run\", parameters.keys())\n",
        "    runs=[]\n",
        "    for v in product(*parameters.values()):\n",
        "      runs.append(Run(*v))\n",
        "    return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyhHndHaOgaQ",
        "colab_type": "text"
      },
      "source": [
        "### Making Run Manager Class to declutter training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfWrmWkTOwmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "class RunManager():\n",
        "  def __init__(self):\n",
        "    \n",
        "    self.epoch_count=0\n",
        "    self.epoch_loss=0\n",
        "    self.epoch_num_correct=0\n",
        "    self.epoch_start_time=None\n",
        "\n",
        "    self.run_params=None # From run builder class\n",
        "    self.run_count=0\n",
        "    self.run_data=[]\n",
        "    self.run_start_time=None\n",
        "\n",
        "    self.network=None\n",
        "    self.loader=None\n",
        "    self.tb=None\n",
        "\n",
        "  def begin_run(self, run, network, loader):\n",
        "    self.run_start_time=time.time()\n",
        "\n",
        "    self.run_params=run\n",
        "    self.run_count+=1\n",
        "\n",
        "    self.network=network\n",
        "    self.loader=loader\n",
        "    self.tb=SummaryWriter(comment=str(run))\n",
        "    \n",
        "    images, labels=next(iter(self.loader))\n",
        "    grid=torchvision.utils.make_grid(images)\n",
        "\n",
        "    self.tb.add_image('images', grid)\n",
        "    self.tb.add_graph(self.network, images)\n",
        "\n",
        "  def end_run(self):\n",
        "    self.tb.close()\n",
        "    self.epoch_count=0\n",
        "\n",
        "  def begin_epoch(self):\n",
        "    self.epoch_start_time=time.time()\n",
        "\n",
        "    self.epoch_count+=1\n",
        "    self.epoch_loss=0\n",
        "    self.epoch_num_correct=0\n",
        "\n",
        "\n",
        "  def end_epoch(self):\n",
        "    epoch_duration=time.time()-self.epoch_start_time\n",
        "    run_duration=time.time()-self.run_start_time\n",
        "\n",
        "    loss=self.epoch_loss/len(self.loader.dataset)\n",
        "    accuracy=self.epoch_num_correct/len(self.loader.dataset)\n",
        "\n",
        "    self.tb.add_scalar(\"Loss\", loss, self.epoch_count)\n",
        "    self.tb.add_scalar(\"Accuracy\", accuracy, self.epoch_count)\n",
        "    \n",
        "    for name, param in self.network.named_parameters():\n",
        "        self.tb.add_histogram(name, param,self.epoch_count)\n",
        "        self.tb.add_histogram(str(name)+\".grad\", param.grad, self.epoch_count)\n",
        "  \n",
        "    results=OrderedDict()\n",
        "    results[\"run\"]=self.run_count\n",
        "    results[\"epoch\"]=self.epoch_count\n",
        "    results[\"loss\"]=loss\n",
        "    results[\"accuracy\"]=accuracy\n",
        "    results[\"epoch duration\"]=epoch_duration\n",
        "    results[\"run duration\"]=run_duration\n",
        "\n",
        "    for index, value in self.run_params._asdict().items(): results[index]=value\n",
        "    self.run_data.append(results)\n",
        "    df=pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(df)\n",
        "    \n",
        "  def track_loss(self, loss):\n",
        "    self.epoch_loss +=loss.item()*self.loader.batch_size\n",
        "\n",
        "  def track_num_correct(self, preds, labels):\n",
        "    self.epoch_num_correct+=self._get_num_correct(preds, labels)\n",
        "\n",
        "  def _get_num_correct(self, preds,labels): # _ here tells that this function is internal\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
        "\n",
        "  def save(self, fileName):\n",
        "    pd.DataFrame.from_dict(\n",
        "        self.run_data,\n",
        "        orient='columns'\n",
        "\n",
        "    ).to_csv(str(fileName)+\".csv\")\n",
        "\n",
        "    with open(str(fileName)+\".json\", 'w', encoding='utf8') as f:\n",
        "      json.dump(self.run_data, f, ensure_ascii=False, indent=4)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab_type": "code",
        "outputId": "8cd9d4c4-7811-4fb3-f696-064c4f2f4662",
        "id": "NzyXYoJwge_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        }
      },
      "source": [
        "parameters=dict(\n",
        "    lr=[0.01,0.001],\n",
        "    batch_size=[100,10000],\n",
        "    shuffle=[False]\n",
        ")\n",
        "\n",
        "m=RunManager()\n",
        "\n",
        "for run in RunBuilder.get_runs(parameters):\n",
        "  # comment=str(run)\n",
        "\n",
        "  lr=run.lr,\n",
        "  batch_size=run.batch_size\n",
        "  shuffle=run.shuffle\n",
        "\n",
        "  network=Network()\n",
        "\n",
        "  train_loader=torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "    )\n",
        "  \n",
        "  optimizer=optim.Adam(network.parameters(), lr=run.lr)\n",
        "\n",
        "\n",
        "  m.begin_run(run,network, train_loader)\n",
        "\n",
        "  for epoch in range(10):\n",
        "      m.begin_epoch()\n",
        "\n",
        "      for batch in train_loader:\n",
        "          images, labels =batch\n",
        "\n",
        "          preds=network(images) # Predict from Batch\n",
        "          loss=F.cross_entropy(preds, labels)# This has the all the data to for back prop\n",
        "\n",
        "          optimizer.zero_grad() # Needed becuase Pytorch makes gradiesnt cumulative, which we do not want\n",
        "          loss.backward() # Back-prop to find gradients\n",
        "          optimizer.step() # Updating weights\n",
        "\n",
        "          m.track_loss(loss)\n",
        "          m.track_num_correct(preds, labels)\n",
        "\n",
        "      m.end_epoch()\n",
        "  m.end_run()\n",
        "m.save('results')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run</th>\n",
              "      <th>epoch</th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>epoch duration</th>\n",
              "      <th>run duration</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>shuffle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.580533</td>\n",
              "      <td>0.780917</td>\n",
              "      <td>15.603108</td>\n",
              "      <td>15.734402</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.397115</td>\n",
              "      <td>0.853383</td>\n",
              "      <td>15.610115</td>\n",
              "      <td>31.470522</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.366559</td>\n",
              "      <td>0.865183</td>\n",
              "      <td>15.706176</td>\n",
              "      <td>47.299625</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.349813</td>\n",
              "      <td>0.870133</td>\n",
              "      <td>16.224739</td>\n",
              "      <td>63.635817</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.338532</td>\n",
              "      <td>0.872683</td>\n",
              "      <td>15.845201</td>\n",
              "      <td>79.621323</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0.332038</td>\n",
              "      <td>0.876700</td>\n",
              "      <td>15.757856</td>\n",
              "      <td>95.493752</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0.325596</td>\n",
              "      <td>0.877250</td>\n",
              "      <td>15.843639</td>\n",
              "      <td>111.451986</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.317555</td>\n",
              "      <td>0.881017</td>\n",
              "      <td>16.042895</td>\n",
              "      <td>127.610321</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0.316850</td>\n",
              "      <td>0.882317</td>\n",
              "      <td>16.110704</td>\n",
              "      <td>143.858102</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0.307288</td>\n",
              "      <td>0.886967</td>\n",
              "      <td>16.092486</td>\n",
              "      <td>160.065431</td>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2.076354</td>\n",
              "      <td>0.239983</td>\n",
              "      <td>16.906420</td>\n",
              "      <td>24.975656</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.503181</td>\n",
              "      <td>0.463500</td>\n",
              "      <td>14.272684</td>\n",
              "      <td>39.389095</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1.060533</td>\n",
              "      <td>0.621067</td>\n",
              "      <td>14.231501</td>\n",
              "      <td>53.739658</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.847645</td>\n",
              "      <td>0.697383</td>\n",
              "      <td>14.212371</td>\n",
              "      <td>68.094044</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0.728735</td>\n",
              "      <td>0.724150</td>\n",
              "      <td>14.221729</td>\n",
              "      <td>82.450425</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0.663928</td>\n",
              "      <td>0.745450</td>\n",
              "      <td>14.138293</td>\n",
              "      <td>96.714488</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.616135</td>\n",
              "      <td>0.760833</td>\n",
              "      <td>14.216575</td>\n",
              "      <td>111.070999</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0.582528</td>\n",
              "      <td>0.771233</td>\n",
              "      <td>14.288975</td>\n",
              "      <td>125.494894</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0.554013</td>\n",
              "      <td>0.783733</td>\n",
              "      <td>14.225665</td>\n",
              "      <td>139.847053</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0.530921</td>\n",
              "      <td>0.794150</td>\n",
              "      <td>14.259471</td>\n",
              "      <td>154.236718</td>\n",
              "      <td>0.010</td>\n",
              "      <td>10000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.783998</td>\n",
              "      <td>0.705250</td>\n",
              "      <td>16.209278</td>\n",
              "      <td>16.347098</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.513197</td>\n",
              "      <td>0.806383</td>\n",
              "      <td>16.335872</td>\n",
              "      <td>32.807207</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.442324</td>\n",
              "      <td>0.840417</td>\n",
              "      <td>16.333021</td>\n",
              "      <td>49.272358</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0.395644</td>\n",
              "      <td>0.857667</td>\n",
              "      <td>16.357581</td>\n",
              "      <td>65.752510</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.363170</td>\n",
              "      <td>0.868150</td>\n",
              "      <td>16.300239</td>\n",
              "      <td>82.174165</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>0.339894</td>\n",
              "      <td>0.875917</td>\n",
              "      <td>16.379272</td>\n",
              "      <td>98.675150</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>0.321680</td>\n",
              "      <td>0.881917</td>\n",
              "      <td>16.286227</td>\n",
              "      <td>115.084760</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0.307202</td>\n",
              "      <td>0.886900</td>\n",
              "      <td>16.286128</td>\n",
              "      <td>131.502244</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>0.295195</td>\n",
              "      <td>0.890267</td>\n",
              "      <td>16.289384</td>\n",
              "      <td>147.916499</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>0.284849</td>\n",
              "      <td>0.895000</td>\n",
              "      <td>16.316658</td>\n",
              "      <td>164.359004</td>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    run  epoch      loss  accuracy  ...  run duration     lr  batch_size  shuffle\n",
              "0     1      1  0.580533  0.780917  ...     15.734402  0.010         100    False\n",
              "1     1      2  0.397115  0.853383  ...     31.470522  0.010         100    False\n",
              "2     1      3  0.366559  0.865183  ...     47.299625  0.010         100    False\n",
              "3     1      4  0.349813  0.870133  ...     63.635817  0.010         100    False\n",
              "4     1      5  0.338532  0.872683  ...     79.621323  0.010         100    False\n",
              "5     1      6  0.332038  0.876700  ...     95.493752  0.010         100    False\n",
              "6     1      7  0.325596  0.877250  ...    111.451986  0.010         100    False\n",
              "7     1      8  0.317555  0.881017  ...    127.610321  0.010         100    False\n",
              "8     1      9  0.316850  0.882317  ...    143.858102  0.010         100    False\n",
              "9     1     10  0.307288  0.886967  ...    160.065431  0.010         100    False\n",
              "10    2      1  2.076354  0.239983  ...     24.975656  0.010       10000    False\n",
              "11    2      2  1.503181  0.463500  ...     39.389095  0.010       10000    False\n",
              "12    2      3  1.060533  0.621067  ...     53.739658  0.010       10000    False\n",
              "13    2      4  0.847645  0.697383  ...     68.094044  0.010       10000    False\n",
              "14    2      5  0.728735  0.724150  ...     82.450425  0.010       10000    False\n",
              "15    2      6  0.663928  0.745450  ...     96.714488  0.010       10000    False\n",
              "16    2      7  0.616135  0.760833  ...    111.070999  0.010       10000    False\n",
              "17    2      8  0.582528  0.771233  ...    125.494894  0.010       10000    False\n",
              "18    2      9  0.554013  0.783733  ...    139.847053  0.010       10000    False\n",
              "19    2     10  0.530921  0.794150  ...    154.236718  0.010       10000    False\n",
              "20    3      1  0.783998  0.705250  ...     16.347098  0.001         100    False\n",
              "21    3      2  0.513197  0.806383  ...     32.807207  0.001         100    False\n",
              "22    3      3  0.442324  0.840417  ...     49.272358  0.001         100    False\n",
              "23    3      4  0.395644  0.857667  ...     65.752510  0.001         100    False\n",
              "24    3      5  0.363170  0.868150  ...     82.174165  0.001         100    False\n",
              "25    3      6  0.339894  0.875917  ...     98.675150  0.001         100    False\n",
              "26    3      7  0.321680  0.881917  ...    115.084760  0.001         100    False\n",
              "27    3      8  0.307202  0.886900  ...    131.502244  0.001         100    False\n",
              "28    3      9  0.295195  0.890267  ...    147.916499  0.001         100    False\n",
              "29    3     10  0.284849  0.895000  ...    164.359004  0.001         100    False\n",
              "\n",
              "[30 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}